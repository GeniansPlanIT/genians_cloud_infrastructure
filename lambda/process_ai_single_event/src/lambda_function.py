import os
import json
import time
import logging
import math
from datetime import datetime, timedelta, timezone

import boto3
from elasticsearch import Elasticsearch, helpers, ConnectionError, TransportError
from elastic_transport import ConnectionTimeout
from openai import OpenAI

# ---- ë¡œê¹… ì„¤ì • ----
logger = logging.getLogger()
logger.setLevel(os.getenv("LOG_LEVEL", "INFO").upper())

# ---- ê¸€ë¡œë²Œ í´ë¼ì´ì–¸íŠ¸ ----
ssm = boto3.client('ssm')
secrets_client = boto3.client('secretsmanager')
es_client = None
openai_client = None

# ---- í—¬í¼ í•¨ìˆ˜ ----
def get_parameter(ssm_client, name, with_decryption=True):
    try:
        response = ssm_client.get_parameter(Name=name, WithDecryption=with_decryption)
        return response['Parameter']['Value']
    except Exception as e:
        logger.error(f"SSMì—ì„œ '{name}' íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", exc_info=True)
        raise e

def initialize_clients():
    """ES ë° OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì¬ì‹œë„ ë° ì•ˆì •ì„± ê°•í™”)"""
    global es_client, openai_client
    if es_client is None or openai_client is None:
        logger.info("Initializing ES and OpenAI clients...")

        ES_HOST = get_parameter(ssm, '/planit/llm/es-host')
        ES_USER = get_parameter(ssm, '/planit/es-user/super')
        ES_PASSWORD = get_parameter(ssm, '/planit/es-password/super')

        # Elasticsearch ì•ˆì •í™” ì˜µì…˜
        es_client = Elasticsearch(
            hosts=[ES_HOST],
            basic_auth=(ES_USER, ES_PASSWORD),
            request_timeout=60,
            max_retries=5,
            retry_on_timeout=True
        )

        SECRET_NAME = get_parameter(ssm, '/planit/llm/secret-name')
        secret = json.loads(secrets_client.get_secret_value(SecretId=SECRET_NAME)['SecretString'])
        openai_client = OpenAI(api_key=secret['OPENAI_API_KEY'])
        logger.info("Clients initialized successfully.")

def fetch_document_with_retry(index, doc_id, retries=3, delay=2):
    """ES ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° ì¬ì‹œë„!"""
    for attempt in range(retries):
        try:
            return es_client.get(index=index, id=doc_id)
        except (ConnectionTimeout, ConnectionError, TransportError) as e:
            logger.warning(f"ES connection failed (attempt {attempt+1}/{retries}): {e}")
            time.sleep(delay)
        except Exception as e:
            logger.error(f"Unexpected error fetching ES document: {e}", exc_info=True)
            break
    return None

# related_eventsì˜ ê°’ì´ ì—†ìŒ ëª…í™•íˆ í‘œì‹œ 
def clean_data_for_llm(data, placeholders=None):
    """
    ë°ì´í„°ë¥¼ LLMì— ì „ë‹¬í•˜ê¸° ì „ì— ì¬ê·€ì ìœ¼ë¡œ ì •ë¦¬
    -1, "-", "" ë“± "ì—†ìŒ"ì„ ì˜ë¯¸í•˜ëŠ” ê°’ë“¤ì„ None (JSON 'null')ìœ¼ë¡œ í†µì¼
    """
    if placeholders is None:
        # "ì—†ìŒ"ì„ ì˜ë¯¸í•˜ëŠ” ê°’ë“¤. í•„ìš”ì— ë”°ë¼ "N/A" ë“±ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŒ
        placeholders = [-1, "-", ""]

    if isinstance(data, dict):
        # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°, ê° valueì— ëŒ€í•´ ì¬ê·€ í˜¸ì¶œ
        return {k: clean_data_for_llm(v, placeholders) for k, v in data.items()}
    
    if isinstance(data, list):
        # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°, ê° í•­ëª©ì— ëŒ€í•´ ì¬ê·€ í˜¸ì¶œ
        return [clean_data_for_llm(item, placeholders) for item in data]

    # ê°’(str, int, float ë“±)ì¸ ê²½ìš°
    if data in placeholders:
        return None  # Noneìœ¼ë¡œ í†µì¼
    
    return data


# ì£¼ë³€ ì´ë²¤íŠ¸ ê²€ìƒ‰ì„ ìœ„í•œ í•¨ìˆ˜
def fetch_related_events(es_client, hostname, timestamp_str, main_event_id, index_pattern="edr-syslog-fixed*", window_seconds=60, size=20):
    """
    ì£¼ì–´ì§„ í˜¸ìŠ¤íŠ¸ì™€ ì‹œê°„ëŒ€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì£¼ë³€ ë¡œê·¸ ESì— ê²€ìƒ‰
    """
    if not hostname or not timestamp_str:
        logger.warning("Cannot fetch related events: HostName or @timestamp is missing from main event.")
        return []

    try:
        # 'Z' (UTC)ë¥¼ íŒŒì´ì¬ì´ ì¸ì‹ ê°€ëŠ¥í•œ '+00:00'ìœ¼ë¡œ ë³€ê²½
        event_time = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
    except (ValueError, TypeError):
        logger.error(f"Invalid @timestamp format, cannot parse: {timestamp_str}")
        return []

    start_time = (event_time - timedelta(seconds=window_seconds)).isoformat()
    end_time = (event_time + timedelta(seconds=window_seconds)).isoformat()

    # ì£¼ë³€ ë¡œê·¸ ê²€ìƒ‰ ì¿¼ë¦¬
    query_body = {
        "query": {
            "bool": {
                "must": [
                    {"match": {"edr.HostName": hostname}}, # .keyword ëŒ€ì‹  match ì‚¬ìš© (í•„ë“œ íƒ€ì… ìœ ì—°ì„±)
                    {"range": {"@timestamp": {"gte": start_time, "lte": end_time}}}
                ],
                # ë…¸ì´ì¦ˆ ì œê±°: ë¬¸ë§¥ íŒŒì•…ì— ì¤‘ìš”í•œ ì´ë²¤íŠ¸ íƒ€ì…ë§Œ í•„í„°ë§
                "filter": [
                    {
                        "bool": {
                            "should": [
                                {"match": {"edr.EventType": "process"}},  # í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰/ì¢…ë£Œ
                                {"match": {"edr.EventType": "network"}},  # í†µì‹  ì‹œë„
                                {"match": {"edr.EventType": "file"}},     # íŒŒì¼ ìƒì„±/ì‚­ì œ (ëœì„¬ì›¨ì–´ ë“±)
                                {"match": {"edr.EventType": "registry"}}  # ìë™ ì‹¤í–‰ ë“±ë¡
                            ],
                            "minimum_should_match": 1
                        }
                    }
                ]
            }
        },
        "sort": [{"@timestamp": {"order": "asc"}}], # ì‹œê°„ìˆœ ì •ë ¬
        # í”„ë¡¬í”„íŠ¸ í¬ê¸°ë¥¼ ì¤„ì´ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•œì˜ í•„ë“œë§Œ ìš”ì²­
        "_source": [
            # 1. í•„ìˆ˜ ì‹ë³„ ì •ë³´
            "@timestamp",
            "edr.HostName",
            "edr.ProcUserID",
            
            # 2. ì´ë²¤íŠ¸ ë¶„ë¥˜
            "edr.EventType",
            "edr.EventSubType",
            
            # 3. í”„ë¡œì„¸ìŠ¤ í–‰ìœ„
            "edr.ProcName",
            "edr.ProcPath",
            "edr.CmdLine",
            "edr.ParentProcName",
            "edr.ParentProcPath",
            "edr.ParentProcCmdLine",

            # 4. ë„¤íŠ¸ì›Œí¬ í–‰ìœ„
            "edr.Direction",
            "edr.RemoteIP",
            "edr.RemotePort",
            "edr.DNSName",
            
            # 5. íŒŒì¼/ë ˆì§€ìŠ¤íŠ¸ë¦¬ í–‰ìœ„
            "edr.FileName",
            "edr.FilePath",
            "edr.RegKeyPath",
            "edr.RegValueName"
        ],
        "size": size
    }

    # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
    # Lambdaì˜ CloudWatch ë¡œê·¸ì—ì„œ ì´ ê°’ë“¤ì„ í™•ì¸í•˜ê¸°
    logger.info(f"[DEBUG] Related Events Query - HostName: {hostname}")
    logger.info(f"[DEBUG] Related Events Query - Time Range: {start_time} to {end_time}")

    # Kibanaì—ì„œ ì‚¬ìš©í•  ì¿¼ë¦¬ ë³¸ë¬¸ ì „ì²´ë¥¼ JSON ë¬¸ìì—´ë¡œ ì¶œë ¥
    logger.info(f"[DEBUG] Related Events Query - ES Body: {json.dumps(query_body, ensure_ascii=False)}")

    try:
        response = es_client.search(
            index=index_pattern, # EDR, Syslog ë“± ê´€ë ¨ ë¡œê·¸ê°€ ìˆëŠ” ëª¨ë“  ì¸ë±ìŠ¤ íŒ¨í„´
            body=query_body
        )
        # _source ë°ì´í„°ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        return [hit['_source'] for hit in response['hits']['hits']]
    except (ConnectionTimeout, ConnectionError, TransportError) as e:
        logger.warning(f"ES search for related events failed: {e}")
        return [] # ì£¼ë³€ ë¡œê·¸ ê²€ìƒ‰ì— ì‹¤íŒ¨í•´ë„ ë©”ì¸ ì´ë²¤íŠ¸ ì²˜ë¦¬ëŠ” ê³„ì†ë˜ë„ë¡ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    except Exception as e:
        logger.error(f"Unexpected error fetching related events: {e}", exc_info=True)
        return []


def call_openai_api(current_openai_client, doc_source, related_events=None):
    """OpenAI API í˜¸ì¶œ(self-evaluation)"""

    if related_events is None:
        related_events = []

    detect_desc = doc_source.get("DetectDesc", {})

    # í•µì‹¬ í•„ë“œ ì„ ë³„
    main_event_context = {
        "NATIP": doc_source.get("NATIP"),
        "HostName": doc_source.get("HostName"),
        "IP": doc_source.get("IP"),
        "AuthName": doc_source.get("AuthName"),
        "Platform": doc_source.get("Platform"),

        "EventTime": doc_source.get("EventTime"), 
        "EventType": doc_source.get("EventType"),
        "EventSubType": doc_source.get("EventSubType"),
        "ProcName": doc_source.get("ProcName"), 
        "FileName": doc_source.get("FileName"),
        "CmdLine": doc_source.get("CmdLine"),   
        "ProcPath": doc_source.get("ProcPath"),
        "DetectTime": doc_source.get("DetectTime"),

        "PathInfo": doc_source.get("PathInfo"),
        "PathInfo2": doc_source.get("PathInfo2"),

        # Persistence ê³µê²© íŒë‹¨ì„ ìœ„í•œ ë ˆì§€ìŠ¤íŠ¸ë¦¬ 3ëŒ€ì¥
        "RegKeyPath": detect_desc.get("RegKeyPath") or doc_source.get("RegKeyPath"),
        "RegValueName": detect_desc.get("RegValueName") or doc_source.get("RegValueName"),
        "RegValue": detect_desc.get("RegValue") or doc_source.get("RegData"),

        # Why (Detection)
        "RuleName": doc_source.get("RuleName"),
        "RuleID": doc_source.get("RuleID"),
        "DetectSubType": doc_source.get("DetectSubType"),
        "ThreatID": doc_source.get("ThreatID"),
        # "IsKnown": doc_source.get("IsKnown"),

        # Why (MITRE)
        "Tactic": doc_source.get("Tactic"),
        "TacticID": doc_source.get("TacticID"),
        "Technique": doc_source.get("Technique"),
        "TechniqueID": doc_source.get("TechniqueID"),

        # What (Detailed Context - Nested Objects)
        "ResponseInfo": doc_source.get("ResponseInfo"),
        "SuspiciousInfo": doc_source.get("SuspiciousInfo"),
        "SuspiciousInfo2": doc_source.get("SuspiciousInfo2")

    }

    # # ë¹ˆ ê°’(None, "")  ì œê±°
    # main_event_context = {k: v for k, v in main_event_context.items() if v not in [None, ""]}

    # -1, "-", "" ë“±ì„ ëª¨ë‘ None (null)ìœ¼ë¡œ í†µì¼
    cleaned_main_event = clean_data_for_llm(main_event_context)
    
    # related_events ë¦¬ìŠ¤íŠ¸ ì „ì²´ì—ë„ ì¬ê·€ì ìœ¼ë¡œ ì ìš©
    cleaned_related_events = clean_data_for_llm(related_events)

    # í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©í•  ì „ì²´ ë°ì´í„° êµ¬ì¡°
    context_data = {
        "main_event": cleaned_main_event,
        "related_events_60sec_window": cleaned_related_events # fetch_related_eventsì—ì„œ ì´ë¯¸ _sourceë§Œ ì¶”ì¶œë¨
    }

    # í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
    prompt = f"""
    You are a Tier 3 Threat Hunter specialized in **Contextual Anomaly Detection**.
    
    **Your Core Objective:**
    Distinguish between **True Threats (APT, Malware)** and **Benign Administrative Activities (Software deployment, Debugging, Remote management)**.
    - Missing a real threat is bad (False Negative).
    - But flooding the SOC with false alarms on admin activity is ALSO bad (False Positive).
    - **Balance is key.**

    **ğŸš¨ CRITICAL OVERRIDE RULE - SIMULATIONS & DRILLS:**
    - If you see indicators of **Atomic Red Team**, **Red Canary**, **Breach and Attack Simulation (BAS)**, or command lines explicitly mentioning "Test", "Simulation", "victim" (e.g., `curl ... atomic-red-team ...`):
        1. **Result:** MUST be **1 (Malicious)**. (We need to prove detection).
        2. **Confidence:** MUST be **Low (60-65)**.
        3. **Reason:** State clearly "Detected Security Simulation / Drill Activity".
    - **Why?** This allows the SOC to see the alert (Result 1) but prioritize it lower than real APT attacks (Confidence 90+).

    **Analysis Process (Chain of Thought):**
    1.  **Analyze Context:** Look at the `main_event` within the `related_events_60sec_window`.
    2.  **Devil's Advocate (Crucial):** Before deciding, force yourself to find reasons why your initial gut feeling might be WRONG.
        - If it looks malicious: Ask "Could this be a clumsy admin or a weird scheduled task?"
        - If it looks normal: Ask "Could this be a stealthy attacker blending in (Living off the Land)?"
    3.  **Verdict:** Only decide after weighing both sides.
    4.  **Scoring:** Assign confidence strictly based on the rubric below.

    **Critical Instructions for Classification:**
    1.  **Analyze Context:** Analyze the `main_event` in the *context* of the `related_events_60sec_window`.
    2.  **'Normal' vs 'Malicious':** A single admin tool (like powershell.exe) in `main_event` might look suspicious alone. But if the `related_events` show normal preceding activity (like logging in, opening admin tools), it is likely 'normal'.
    3.  **'Malicious' Indicators:** Look for suspicious *sequences*, like Office apps (winword.exe, excel.exe) launching powershell.exe, or downloads followed by execution from temp folders.

    ---
    **Analyze the following data package:**
    (The `related_events_60sec_window` shows other logs from the same host +-60 seconds around the `main_event`)
    {json.dumps(context_data, ensure_ascii=False, indent=2)}

    ---
    Classify the `main_event` (NOT the related events) as 'malicious' or 'normal' and explain your reasoning *based on the full context*.

    ---
    Classify the `main_event` using the following codes:
    - **0**: Normal (Safe, Benign)
    - **1**: Malicious (Threat, Suspicious)

    ---
    **Your Task:**
    1. Determine if the event is Normal (0) or Malicious (1).
    2. **Self-Evaluate your confidence score (0-100)** based on the evidence strength.

    **Confidence Scoring Rubric (Strict enforcement):**
    - **95-100:** Absolute Certainty. Matches a known APT signature, hash, or exact MITRE attack pattern with NO benign explanation.
    - **80-94:** High Confidence. Strong anomaly (e.g., encoded PowerShell, credential dumping attempt) but theoretically possible by an admin.
    - **60-79:** Suspicious. Weird behavior, but lacks context or could be a false positive.
    - **0-59:** Low Confidence. Insufficient data, generic logs, or highly likely to be noise.

    Respond ONLY in this JSON format:
    {{
    "result": 0, // Use integer 0 or 1 only
    "reason": "The core reason for your judgment, referencing the main_event and any relevant context from related_events",
    "confidence": 95 // Your self-evaluated score (Integer 0-100)
    }}
    """

    # ---- í”„ë¡¬í”„íŠ¸ ì¶œë ¥ ì¶”ê°€! ----
    print("==== Generated Prompt(with Context) ====")
    print(prompt)
    print("==========================")
    
    try:
        response = current_openai_client.chat.completions.create(
            model="gpt-5",
            messages=[
                {"role": "system", "content": "You are a top-tier cybersecurity analyst."},
                {"role": "user", "content": prompt}
            ],
            # Structured Outputs (Schema)
            response_format={
                "type": "json_schema",
                "json_schema": {
                    "name": "threat_classification",
                    "strict": True, # ìŠ¤í‚¤ë§ˆ ì—„ê²© ì¤€ìˆ˜ ëª¨ë“œ
                    "schema": {
                        "type": "object",
                        "properties": {
                            # 1. ë¶„ì„ ë‚´ìš© ìš”ì•½
                            "analysis_summary": {
                                "type": "string",
                                "description": "Brief summary of what happened."
                            },
                            # 2. ë°˜ëŒ€ ì¦ê±° ê°•ì œ (ì•…ë§ˆì˜ ë³€í˜¸ì¸)
                            # ì´ í•„ë“œë¥¼ ì±„ìš°ë©´ì„œ ëª¨ë¸ì€ ìì‹ ì˜ í™•ì‹ ì„ ë‚®ì¶”ê²Œ ë¨
                            "counter_evidence": {
                                "type": "string",
                                "description": "List reasons why the OPPOSITE verdict might be true. (e.g., if you think it's malicious, list why it could be normal)"
                            },
                            "result": {
                                "type": "integer",
                                "enum": [0, 1], # 0 ë˜ëŠ” 1ë§Œ í—ˆìš© (Enum)
                                "description": "0 for Normal, 1 for Malicious"
                            },
                            "reason": {
                                "type": "string",
                                "description": "Reasoning for the classification"
                            },
                            "confidence": {
                                "type": "integer",
                                "description": "Confidence score between 0 and 100 based on evidence strength",
                                "minimum": 0,
                                "maximum": 100
                            }
                        },
                        "required": ["analysis_summary", "counter_evidence", "result", "reason", "confidence"],
                        "additionalProperties": False
                    }
                }
            }
        )

        analysis_content = response.choices[0].message.content
        parsed_json = json.loads(analysis_content)

        result_code = parsed_json.get("result") # 0 ë˜ëŠ” 1

        confidence_score = float(parsed_json.get("confidence", 0))
            
        # íŠœí”Œë¡œ ë°˜í™˜: (JSONê²°ê³¼, ì‹ ë¢°ë„ì ìˆ˜)
        return parsed_json, confidence_score

    except Exception as e:
        logger.error("OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ", exc_info=True)
        return None, 0.0

def process_messages(records):
    """SQS ë©”ì‹œì§€ ì²˜ë¦¬ ë° Elasticsearch bulk indexing"""
    failed_message_ids = []
    actions_to_index = []

    KST = timezone(timedelta(hours=9))
    current_datetime_str = datetime.now(tz=KST).strftime("%Y.%m.%d_%H")
    # dest_index = f"planit-edr-ai-analyzed-nmal5-{current_datetime_str}"
    dest_index = "planit-llm-malicious"
    analysis_time = datetime.now().isoformat() # ì¼ê´€ëœ ë¶„ì„ ì‹œê°„ì„ ìœ„í•´ ë¯¸ë¦¬ ì •ì˜

    for record in records:
        payload = None
        unique_id = "N/A" # ì˜¤ë¥˜ ë¡œí‚¹ì„ ìœ„í•´ ID ë³€ìˆ˜ ë¯¸ë¦¬ ì„ ì–¸ 

        try:
            payload = json.loads(record['body'])
            unique_id = payload.get('UniqueID')
            source_index = payload.get('SourceIndex')

            if not unique_id or not source_index:
                logger.warning(f"Message missing UniqueID or SourceIndex. Skipping.")
                continue

            # ES ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
            doc = fetch_document_with_retry(source_index, unique_id)
            if not doc:
                raise Exception(f"Failed to fetch document '{unique_id}' from ES")
            doc_source = doc['_source']

            # 1.5 ì‹œê°„ì  ë§¥ë½ì„ ìœ„í•´ ì£¼ë³€ ì´ë²¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            event_hostname = doc_source.get("HostName")
            event_timestamp = doc_source.get("@timestamp") # '@timestamp' í•„ë“œ ì‚¬ìš©

            # Syslog ê°€ì ¸ì˜´
            related_events_index_pattern = "edr-syslog-fixed*"

            related_events = fetch_related_events(
                es_client=es_client,
                hostname=event_hostname,
                timestamp_str=event_timestamp,
                main_event_id=unique_id,
                index_pattern=related_events_index_pattern,
                window_seconds=60, # ì „í›„ 60ì´ˆ
                size=50 # ìµœëŒ€ 50ê°œ
            )


            # OpenAI ë¶„ì„
            analysis_result, confidence_score = call_openai_api(openai_client, doc_source, related_events)

            if analysis_result:

                # --- AI ì •í™•ë„ ê²€ì¦ ì½”ë“œ ì‹œì‘ ---

                # 1. AI ì˜ˆì¸¡ê°’(0/1)ì„ ë¬¸ìì—´("normal"/"malicious")ë¡œ ë³€í™˜
                ai_prediction_code = analysis_result.get("result") # 0 or 1 or None
                ai_prediction_str = None
                if ai_prediction_code == 0:
                    ai_prediction_str = "normal"
                elif ai_prediction_code == 1:
                    ai_prediction_str = "malicious"

                # 2. Ground Truth (ì •ë‹µ) ê°€ì ¸ì˜¤ê¸° 
                threat_label = doc_source.get("threat_label", {})
                ground_truth = threat_label.get("verdict") if isinstance(threat_label, dict) else None

                # 3. AI ì˜ˆì¸¡ê³¼ Ground Truth ë¹„êµ
                is_correct = None
                if ai_prediction_str is not None and ground_truth is not None:
                    is_correct = (ai_prediction_str == str(ground_truth).lower()) 

                # 3. ë¡œê·¸ ìƒì„¸ ì¶œë ¥
                is_correct_str = "N/A (Missing Data)"
                if is_correct is True:
                    is_correct_str = "âœ… CORRECT"
                elif is_correct is False:
                    is_correct_str = "âŒ INCORRECT"
                
                logger.info(
                    f"AI Accuracy Check [ID: {unique_id}]: "
                    f"Prediction='{ai_prediction_str}' (Code: {ai_prediction_code}), "
                    f"GroundTruth='{ground_truth}'. "
                    f"Result: {is_correct_str}"
                )

                # --- AI ì •í™•ë„ ê²€ì¦ ì½”ë“œ ë ---

                # 4. ë¶„ì„ ê²°ê³¼ë¥¼ doc_sourceì— ì €ì¥ (ì •í™•ë„ í•„ë“œ í¬í•¨)
                doc_source['ai_analysis'] = {
                    "result": ai_prediction_str,
                    "result_code": ai_prediction_code, 
                    "reason": analysis_result.get("reason"),
                    "confidence": confidence_score, # ê³„ì‚°ëœ ì‹ ë¢°ë„ ì €ì¥ 
                    "analyzed_at": analysis_time,
                    "context_events_count": len(related_events), # ë””ë²„ê¹…ì„ ìœ„í•´ ì¶”ê°€
                    "accuracy": { # ì •í™•ë„ ê²°ê³¼ í•„ë“œ ì¶”ê°€
                        "ground_truth_verdict": ground_truth,
                        "is_correct": is_correct
                    }
                }

                actions_to_index.append({
                    "_op_type": "index",
                    "_index": dest_index,
                    "_id": unique_id,
                    "_source": doc_source
                })
            else:
                raise Exception(f"AI analysis result was None for UniqueID '{unique_id}'")

        except Exception as e:
            # unique_id ë³€ìˆ˜ê°€ try ë¸”ë¡ ì´ˆê¸°ì— í• ë‹¹ë˜ì–´ ì˜¤ë¥˜ ë¡œê·¸ì— ID í¬í•¨ ê°€ëŠ¥
            logger.error(f"Failed to process message UniqueID '{unique_id}': {e}", exc_info=True)
            failed_message_ids.append(record['messageId'])

    # Bulk ì¸ë±ì‹±
    batch_size = 50
    for i in range(0, len(actions_to_index), batch_size):
        batch = actions_to_index[i:i+batch_size]
        try:
            success, failed = helpers.bulk(
                es_client,
                batch,
                raise_on_error=False,
                stats_only=True
            )
            logger.info(f"Bulk batch: {success} successful, {failed} failed")
        except (ConnectionTimeout, ConnectionError, TransportError) as e:
            logger.error(f"Bulk indexing network error: {e}", exc_info=True)
            failed_message_ids.extend([r['_id'] for r in batch])
        except Exception as e:
            logger.error(f"Bulk indexing unexpected error: {e}", exc_info=True)
            failed_message_ids.extend([r['_id'] for r in batch])

    return failed_message_ids

# ---- Lambda í•¸ë“¤ëŸ¬(ì£¼ë³€ ì´ë²¤íŠ¸ ê²€ìƒ‰ ë¡œì§ ì¶”ê°€) ----
def lambda_handler(event, context):
    try:
        initialize_clients()
        failed_ids = process_messages(event.get('Records', []))
        return {
            'batchItemFailures': [{'itemIdentifier': msg_id} for msg_id in failed_ids]
        }
    except Exception as e:
        logger.error(f"Lambda handler failed: {e}", exc_info=True)
        raise e